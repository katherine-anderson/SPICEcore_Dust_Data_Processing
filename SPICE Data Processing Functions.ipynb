{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script with function definitions. This way, the commands for each calculation can just be written once.\n",
    "#\n",
    "# Katie Anderson, 5/28/19\n",
    "#\n",
    "# List of functions:\n",
    "#\n",
    "# 1) label_core_breaks:   Get a list of indices for each CFA row near a core break (by depth)\n",
    "# 2) label_volc_events:   Get a list of indices for each row in a volcanic window (by age)\n",
    "# 3) find_cpp:            Calculate CPP for a CFA dataframe\n",
    "# 4) label_dust_events:   Get a list of indices for each row in a dust event (by depth)\n",
    "# 5) find_humps:          Find hump-shaped PSD anomalies in a CFA dataframe\n",
    "# 6) test_scenario:       Create an outlier removal scenario for a CFA dataframe\n",
    "# 7) plot_single_psd:     Create histogram of particle counts around a given depth\n",
    "# 8) summary_statistics:  Print summary statistics for dust concentration & CPP during data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get indices of all CFA measurements taken within a specified core break range\n",
    "# Inputs: CFA dataframe, core breaks dataframe, specified +/- core break range (in meters)\n",
    "# Output: List of rows within core breaks\n",
    "\n",
    "def label_core_breaks(cfa_data, core_breaks, core_range):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements which occurred around a core break range\n",
    "    break_true  = []\n",
    "    # Create an empty list to record the first row within each core break range\n",
    "    new_break = []\n",
    "    \n",
    "    # Subset the CFA data for depths within range of each core break\n",
    "    for corebreak in core_breaks['Depth (m)']:\n",
    "        new_cfa = cfa_data[(cfa_data['Depth (m)'] >= (corebreak - core_range)) & \n",
    "                           (cfa_data['Depth (m)'] <= (corebreak + core_range))]\n",
    "        # Check that there are CFA measurements in the core break interval\n",
    "        if new_cfa.empty: continue  \n",
    "        # Add all depth values in the core break interval to a list\n",
    "        else: \n",
    "            # Add all indices within core breaks to the list\n",
    "            break_true.extend(new_cfa.index.values.tolist())\n",
    "            # Add the first index of the CFA data for one core break to the list\n",
    "            new_break.append(new_cfa.index[0])\n",
    "            \n",
    "    # Return list of indices occurring within core breaks\n",
    "    return break_true, new_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get indices of all CFA measurements taken within range of years around volcanic events\n",
    "# Inputs: Holocene CFA, Holocene volcanic dates, before/after buffers, in years\n",
    "# Output: List of rows within volcanic range\n",
    "\n",
    "def label_volc_events(cfa_data, volc_record, start_buffer, end_buffer):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements which occurred near a volcanic event\n",
    "    volc_true  = []\n",
    "    # Create an empty list to record the first row within each volcanic event\n",
    "    new_volc = []\n",
    "        \n",
    "    # Subset the CFA data for depths within range of other volcanic events\n",
    "    for start_year in volc_record['Start Year (b1950)']:\n",
    "        new_cfa = cfa_data[(cfa_data['Age b 1950'] <= (start_year + start_buffer)) & \n",
    "                           (cfa_data['Age b 1950'] >= (start_year - end_buffer  ))]\n",
    "        # Check that there are CFA measurements in the interval around the volcanic events\n",
    "        if new_cfa.empty: continue  \n",
    "        else: \n",
    "            # Add all indices within volcanic events to the list\n",
    "            volc_true.extend(new_cfa.index.values.tolist())\n",
    "            # Add the first index of the CFA data for one volcanic event to the list\n",
    "            new_volc.append(new_cfa.index[0])\n",
    "            \n",
    "    # Return list of rows within buffer dates of volcanic events\n",
    "    return volc_true, new_volc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate CPP per measurement. Asks the user to keep/exclude smallest and largest bins.\n",
    "# Input: CFA data\n",
    "# Output: List of CPP for each row\n",
    "\n",
    "def find_cpp(cfa_data):\n",
    "    # Create dataframe to record particle sums. Needed to prevent dividing by 0\n",
    "    cpp_df = pd.DataFrame(columns = ['Sum_All', 'Sum_Coarse'])\n",
    "    \n",
    "    # Ask the user to include/exclude smallest & largest bins\n",
    "    choice = input('-->Use smallest and largest bins for CPP? Enter Y or N: ')\n",
    "\n",
    "    # Create column lists for either option\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        col_list = ['1', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10', '12']\n",
    "    if choice == 'n' or choice == 'N':\n",
    "           col_list = ['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10']\n",
    "    # Sum particle counts for each measurement using the above columns\n",
    "    cpp_df['Sum_All'] = cfa_data[col_list].sum(axis = 1)\n",
    "    # Check for negative counts\n",
    "    if min(cpp_df['Sum_All']) < 0: \n",
    "        print('CPP function found negative sum of all particles.')\n",
    "\n",
    "    # Remake the column lists for only the coarse particles (>= 4.5 um)\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        col_list = ['4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10', '12']\n",
    "\n",
    "    if choice == 'n' or choice == 'N':\n",
    "        col_list = ['4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10']\n",
    "\n",
    "    # Sum coarse particle counts for each measurement using the above columns\n",
    "    cpp_df['Sum_Coarse'] = cfa_data[col_list].sum(axis = 1)\n",
    "    # Check for negative counts\n",
    "    if min(cpp_df['Sum_Coarse']) < 0: \n",
    "        print('CPP function found negative sum of coarse particles.')\n",
    "    \n",
    "    # Remove rows with 0 sum_all counts. Don't divide by 0\n",
    "    cpp_df = cpp_df[cpp_df['Sum_All'] > 0]\n",
    "\n",
    "    # Return a series of the percent of particles that are coarse per row\n",
    "    return(cpp_df['Sum_Coarse'] / cpp_df['Sum_All'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dust_events(cfa_data, dust_depths):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements within depth range of dust events\n",
    "    dust_event_true = []\n",
    "    \n",
    "    # Subset the CFA data for depths within range of dust events\n",
    "    for index, row in dust_events.iterrows():\n",
    "        new_cfa = cfa_data[(cfa_data['Depth (m)'] >= row['Dust Event Start (m)']) &\n",
    "                           (cfa_data['Depth (m)'] <= row['Dust Event End (m)'])]\n",
    "        # Check that there are CFA measurements in the dust event depth intervals\n",
    "        if new_cfa.empty: continue\n",
    "        # Add all indices within dust events to the list\n",
    "        else:\n",
    "            dust_event_true.extend(new_cfa.index.values.tolist())\n",
    "            \n",
    "    # Return list of rows within dust events \n",
    "    return dust_event_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies CFA measurements with hump PSD anomalies\n",
    "# Inputs: CFA data and the min/max depths in which to find the humps\n",
    "# Output: CFA dataframe with only the hump measurements\n",
    "# Hump criteria: Measurements where all bins 3.5-10 um have higher counts than the\n",
    "#    average count for bins 1.5-2.9 um.\n",
    "\n",
    "def find_humps(cfa_data, min_depth, max_depth):\n",
    "    \n",
    "    # Subset the CFA data for the selected depth range\n",
    "    cfa_data = cfa_data[(cfa_data['Depth (m)'] >= min_depth) & \n",
    "                        (cfa_data['Depth (m)'] <= max_depth)]\n",
    "    \n",
    "    # Make a copy of CFA data for bins 3.2-10\n",
    "    humps_col_list = ['3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10']\n",
    "    \n",
    "    cfa_humps = cfa_data[humps_col_list]\n",
    "    \n",
    "    # Make copy of CFA data for bins 1.5-2.9\n",
    "    smalls_col_list = ['1.5', '1.6', '1.7', '1.8', '1.9', '2', '2.1', '2.2',\n",
    "                       '2.3', '2.4', '2.5', '2.7', '2.9']\n",
    "    \n",
    "    cfa_smalls = cfa_data[smalls_col_list]  \n",
    "    \n",
    "    # Get mean concentration for bins 1.5-2.9 per row\n",
    "    smalls_mean = cfa_smalls.mean(axis = 'columns')\n",
    "    \n",
    "    # Subtract the 1.5-2.9 bin mean from the 3.2-10 values\n",
    "    cfa_humps = cfa_humps.subtract(smalls_mean, axis = 'index')\n",
    "    \n",
    "    # If all subtracted values are positive, it is a hump\n",
    "    # Mark all positive differences as True\n",
    "    criteria = cfa_humps > 0\n",
    "    # Check for rows with only True values\n",
    "    criteria = criteria.all(axis = 'columns')\n",
    "\n",
    "    # Subset the CFA data for rows where all values are elevated above small particles\n",
    "    cfa_data = cfa_data[criteria]\n",
    "    \n",
    "    # Count the number of discrete humps\n",
    "    # Copy the first two columns of the CFA data into new dataframe\n",
    "    humps_diff = cfa_data.loc[:, 'Depth (m)':'ECM'].copy()\n",
    "    # Subtract each row from the one before (to get diff in depth)\n",
    "    humps_diff = humps_diff.diff()\n",
    "    # Subset the diff dataframe for rows where depth changes by 3+ cm\n",
    "    # ~3 cm melt resolution. >3 cm diff = new hump event\n",
    "    new_humps = humps_diff[(humps_diff['Depth (m)'] >= 0.03)]\n",
    "    \n",
    "    # Report the number of hump measurements and events\n",
    "    print('Number of measurements:    ', len(cfa_data))\n",
    "    print('Number of discrete events: ', len(new_humps))\n",
    "    \n",
    "    return cfa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers given different background & sensitivity conditions\n",
    "# Inputs: CFA data, list of dust event rows, # of measurements for background levels, # of stdevs. accepted\n",
    "# Output: Outlier counts, dataframe with overlapping outliers removed\n",
    "\n",
    "def test_threshold(cfa_data, dust_indices, background_interval, threshold_stdev):\n",
    "    print('\\nNEW OUTLIER REMOVAL SCENARIO')\n",
    "    \n",
    "    # Calculate rolling standard deviations\n",
    "    # Will calculate std. if only 1 measurement in the window that isn't NaN\n",
    "    cpp_background  = cfa_data['CPP'].rolling(background_interval, min_periods = 3).std()\n",
    "    conc_background = cfa_data['Sum 1.1-10'].rolling(background_interval, min_periods = 3).std()   \n",
    "\n",
    "    # Subsetting CFA data for the outliers\n",
    "    cpp_peaks  =  cfa_data[(cfa_data['CPP'] >= ( threshold_stdev *  cpp_background)) ]\n",
    "    conc_peaks =  cfa_data[(cfa_data['Sum 1.1-10'] >= ( threshold_stdev *  conc_background))]\n",
    "\n",
    "    # Want to find when these outliers occur at the same time\n",
    "    overlap = conc_peaks.index.intersection(cpp_peaks.index)\n",
    "    # Prevent rows in real dust events from being removed\n",
    "    overlap = overlap.difference(dust_indices)\n",
    "    \n",
    "    # Ask the user whether or not to display # of outliers found & removed\n",
    "    choice1 = input('\\n-->Print outlier counts? Enter Y or N: ')\n",
    "    if choice1 == 'y' or choice1 == 'Y':    \n",
    "        print('\\nConc. outliers:              ', len(conc_peaks))\n",
    "        print('CPP outliers:                ', len(cpp_peaks))\n",
    "        print('Overlap:                     ', len(overlap))\n",
    "    \n",
    "    if 'Volcanic Event?' in cfa_data.columns:\n",
    "        # Subset the outlier dataframes for rows within volcanic events\n",
    "        volc_conc_peaks = conc_peaks[(conc_peaks['Volcanic Event?'] == True)]\n",
    "        volc_cpp_peaks  = cpp_peaks [(cpp_peaks ['Volcanic Event?'] == True)]\n",
    "\n",
    "        # Count the number of overlapped outliers which occur within a volcanic event\n",
    "        volc_overlap = volc_conc_peaks.index.intersection(volc_cpp_peaks.index)\n",
    "        \n",
    "        if choice1 == 'y' or choice1 == 'Y':\n",
    "            print('\\nConc. outliers w/in volcanic event:', len(volc_conc_peaks))\n",
    "            print('CPP outliers w/in volcanic event:  ', len(volc_cpp_peaks))\n",
    "            print('Overlap w/in volcanic event:       ', len(volc_overlap))\n",
    "    \n",
    "        # Want to have the option of preserving volcanic events or not\n",
    "        choice2 = input('\\n-->Remove outliers at volcanic events? Enter Y or N: ')\n",
    "        if choice2 == 'y' or choice2 == 'Y':\n",
    "            # Remove variable is the indices at which to NaN values\n",
    "            remove = overlap\n",
    "            if choice1 == 'y' or choice1 == 'Y':\n",
    "                print('\\nSUMMARY OF OUTLIER REMOVAL')\n",
    "                print('1) Rows Removed: ', len(remove))\n",
    "        if choice2 == 'n' or choice2 == 'N':\n",
    "            # Subtract the volc_overlap indices from the overlap indices\n",
    "            remove = overlap.difference(volc_overlap)\n",
    "            if choice1 == 'y' or choice1 == 'Y':\n",
    "                print('\\nSUMMARY OF OUTLIER REMOVAL')\n",
    "                print('1) Rows Removed: ', len(remove))\n",
    "    \n",
    "        # Count number of discrete volcanic events in the rows we're about to remove\n",
    "        # Get a copy of the CFA data with only the 'New Volcanic Event?' column\n",
    "        temp_cfa = cfa_data.loc[remove, :].copy()\n",
    "        volc_event = temp_cfa[(temp_cfa['New Volcanic Event?'] == True)]\n",
    "        if choice1 == 'y' or choice1 == 'Y':\n",
    "            print('2) Number of discrete volcanic events in removed rows:', len(volc_event))\n",
    "        \n",
    "        # Count number of core breaks in the rows we're about to remove\n",
    "        temp_cfa = cfa_data.loc[remove, :].copy()\n",
    "        break_outliers = temp_cfa[(temp_cfa['New Break?'] == True)]\n",
    "        if choice1 == 'y' or choice1 == 'Y':\n",
    "            print('3) Number of discrete core breaks in removed rows:    ', len(break_outliers))\n",
    "    else: \n",
    "        # Just remove the rows w/ overlapping CPP and conc. outliers\n",
    "        remove = overlap\n",
    "        \n",
    "        # Count number of core breaks in the rows we're about to remove\n",
    "        temp_cfa = cfa_data.loc[remove, :].copy()\n",
    "        break_outliers = temp_cfa[(temp_cfa['New Break?'] == True)]\n",
    "        \n",
    "        print('\\nSUMMARY OF OUTLIER REMOVAL')\n",
    "        print('1) Rows Removed:', len(remove))\n",
    "        print('2) Number of discrete core breaks in removed rows:', len(break_outliers))\n",
    "        \n",
    "    # Change all rows with overlapping outliers to NaN\n",
    "    # Don't NaN the 'Break?', 'New Break?', 'Volcanic Event?', and 'New Volcanic Event?' columns\n",
    "    cfa_data.loc[remove, ['Depth (m)', 'Age b 1950', 'Flow Rate', 'ECM', '1', '1.1',\n",
    "                            '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9', '2', \n",
    "                            '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', '3.2', '3.6', \n",
    "                            '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10', '12',\n",
    "                            'Original Depth (m)', 'CPP', 'Sum 1.1-10']] = np.nan\n",
    "\n",
    "    return cfa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create 1 bar plot of particle counts within x-centimenters of a point\n",
    "#     Inputs: CFA dataframe, depth of interest, depth range around that point\n",
    "#     Output: Bar plot of particle counts per bin, with option to save\n",
    "\n",
    "def plot_single_psd(cfa_data, point, depth_range):\n",
    "\n",
    "    # The range around the main point of interest\n",
    "    point_min = point - depth_range\n",
    "    point_max = point + depth_range\n",
    "\n",
    "    # Subset the CFA data to range around given point\n",
    "    point_cfa = cfa_data[(cfa_data['Depth (m)'] >= point_min) \n",
    "                        & (cfa_data['Depth (m)'] <= point_max)]\n",
    "    \n",
    "    # Check for and remove columns\n",
    "    col_list = ['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10']\n",
    "    point_cfa = point_cfa[col_list]\n",
    "    \n",
    "    # Sum particles by column around main & comp. points\n",
    "    point_count = point_cfa.sum(axis = 0)\n",
    "    \n",
    "    # Make figures\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "    \n",
    "    ax.bar(col_list, point_count, width = 1, color = 'black');\n",
    "    ax.set_xticks([0,10,20,29])\n",
    "    ax.tick_params(labelsize = 14)\n",
    "    ax.set_ylabel('Counts (#/uL)', fontsize = 16)\n",
    "    ax.set_title(str(point) + ' Meters +/- ' + str(depth_range) + ' Meters', fontsize = 18)    \n",
    "    \n",
    "    choice = input('Save Figure? Enter Y or N: ')\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        os.chdir('C:\\\\Users\\\\katie\\\\OneDrive\\\\Documents\\\\SPICE\\\\PSD Plots')\n",
    "        plt.savefig('PSD_1_' + str(point) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print summary statistics for dust concentration & CPP during data cleaning\n",
    "#     Inputs: CFA dataframe with particle sum and CPP columns\n",
    "#     Output: None. Prints summary statistics.\n",
    "\n",
    "def summary_statistics(cfa_data):\n",
    "    \n",
    "    if 'Sum 1.1-10' in cfa_data.columns and 'CPP' in cfa_data.columns:  \n",
    "        row_sums = cfa_data.loc[:, 'Sum 1.1-10'].copy()\n",
    "        cpp      = cfa_data.loc[:, 'CPP'].copy()\n",
    "        \n",
    "        # Convert number concentration to # / mL\n",
    "        number_conc = row_sums.mul(1000).copy()\n",
    "    \n",
    "        print('SUMMARY STATISTICS')\n",
    "        # Skip NaNs when calculating these\n",
    "        print('Dust number concentration (/mL):')\n",
    "        print('    Mean:   %.2f' % (np.nanmean(number_conc)))\n",
    "        print('    Median: %.2f' % (np.nanmedian(number_conc)))\n",
    "        print('    Min:    %.2f' % (np.nanmin(number_conc)))\n",
    "        print('    Max:    %.2f' % (np.nanmax(number_conc)))\n",
    "        print('    StDev:  %.2f' % (np.nanstd(number_conc)))\n",
    "    \n",
    "        print('\\nCoarse Particles (%):')\n",
    "        print('    Mean:   %.2f' % (np.nanmean(cpp)))\n",
    "        print('    Median: %.2f' % (np.nanmedian(cpp)))\n",
    "        print('    Min:    %.2f' % (np.nanmin(cpp)))\n",
    "        print('    Max:    %.2f' % (np.nanmax(cpp)))\n",
    "        print('    StDev:  %.2f' % (np.nanstd(cpp)))\n",
    "        \n",
    "    else: print('Input data with particle sum and CPP columns')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
