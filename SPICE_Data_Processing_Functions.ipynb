{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script with function definitions. This way, the commands for each calculation can just be written once.\n",
    "#\n",
    "# Katie Anderson, 8/1/19\n",
    "#\n",
    "# List of functions:\n",
    "#\n",
    "#  1) label_core_breaks:         Get a list of indices for each CFA row near a core break (by depth)\n",
    "#  2) label_volc_events:         Get a list of indices for each row in a volcanic window (by age)\n",
    "#  3) find_cpp:                  Calculate CPP for a CFA dataframe\n",
    "#  4) label_dust_events:         Get a list of indices for each row in a dust event (by depth)\n",
    "#  5) find_humps:                Find hump-shaped PSD anomalies in a CFA dataframe\n",
    "#  6) median_absolute_deviation: Calculate MAD for one column of CFA data\n",
    "#  7) remove_outliers_MAD:       Remove outliers from the CFA data, using MAD\n",
    "#  8) plot_single_psd:           Create histogram of particle counts around a given depth\n",
    "#  9) summary_statistics:        Print summary statistics for dust concentration & CPP during data cleaning\n",
    "# 10) remove_outliers_integrals: Get a list of indices with an integral outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get indices of all CFA measurements taken within a specified core break range\n",
    "# Inputs: CFA dataframe, core breaks dataframe, specified +/- core break range (in meters)\n",
    "# Output: List of rows within core breaks\n",
    "\n",
    "def label_core_breaks(cfa_data, core_breaks, core_range):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements which occurred around a core break range\n",
    "    break_true  = []\n",
    "    # Create an empty list to record the first row within each core break range\n",
    "    new_break = []\n",
    "    \n",
    "    # Subset the CFA data for depths within range of each core break\n",
    "    for corebreak in core_breaks['Depth (m)']:\n",
    "        new_cfa = cfa_data[(cfa_data['Depth (m)'] >= (corebreak - core_range)) & \n",
    "                           (cfa_data['Depth (m)'] <= (corebreak + core_range))]\n",
    "        # Check that there are CFA measurements in the core break interval\n",
    "        if new_cfa.empty: continue  \n",
    "        # Add all depth values in the core break interval to a list\n",
    "        else: \n",
    "            # Add all indices within core breaks to the list\n",
    "            break_true.extend(new_cfa.index.values.tolist())\n",
    "            # Add the first index of the CFA data for one core break to the list\n",
    "            new_break.append(new_cfa.index[0])\n",
    "            \n",
    "    # Return list of indices occurring within core breaks\n",
    "    return break_true, new_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get indices of all CFA measurements taken within range of years around volcanic events\n",
    "# Inputs: Holocene CFA, Holocene volcanic dates, before/after buffers, in years\n",
    "# Output: List of rows within volcanic range\n",
    "\n",
    "def label_volc_events(cfa_data, volc_record, start_buffer, end_buffer):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements which occurred near a volcanic event\n",
    "    volc_true  = []\n",
    "    # Create an empty list to record the first row within each volcanic event\n",
    "    new_volc = []\n",
    "        \n",
    "    # Subset the CFA data for depths within range of other volcanic events\n",
    "    for start_year in volc_record['Start Year (b1950)']:\n",
    "        new_cfa = cfa_data[(cfa_data['Age b 1950'] <= (start_year + start_buffer)) & \n",
    "                           (cfa_data['Age b 1950'] >= (start_year - end_buffer  ))]\n",
    "        # Check that there are CFA measurements in the interval around the volcanic events\n",
    "        if new_cfa.empty: continue  \n",
    "        else: \n",
    "            # Add all indices within volcanic events to the list\n",
    "            volc_true.extend(new_cfa.index.values.tolist())\n",
    "            # Add the first index of the CFA data for one volcanic event to the list\n",
    "            new_volc.append(new_cfa.index[0])\n",
    "            \n",
    "    # Return list of rows within buffer dates of volcanic events\n",
    "    return volc_true, new_volc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate CPP per measurement. Asks the user to keep/exclude smallest and largest bins.\n",
    "# Input: CFA data\n",
    "# Output: List of CPP for each row\n",
    "\n",
    "def find_cpp(cfa_data):\n",
    "    # Create dataframe to record particle sums. Needed to prevent dividing by 0\n",
    "    cpp_df = pd.DataFrame(columns = ['Sum_All', 'Sum_Coarse'])\n",
    "    \n",
    "    # Ask the user to include/exclude smallest & largest bins\n",
    "    choice = input('-->Use smallest and largest bins for CPP? Enter Y or N: ')\n",
    "\n",
    "    # Create column lists for either option\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        col_list = ['1', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10', '12']\n",
    "    if choice == 'n' or choice == 'N':\n",
    "           col_list = ['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10']\n",
    "    # Sum particle counts for each measurement using the above columns\n",
    "    cpp_df['Sum_All'] = cfa_data[col_list].sum(axis = 1)\n",
    "    # Check for negative counts\n",
    "    if min(cpp_df['Sum_All']) < 0: \n",
    "        print('CPP function found negative sum of all particles.')\n",
    "\n",
    "    # Remake the column lists for only the coarse particles (>= 4.5 um)\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        col_list = ['4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10', '12']\n",
    "\n",
    "    if choice == 'n' or choice == 'N':\n",
    "        col_list = ['4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10']\n",
    "\n",
    "    # Sum coarse particle counts for each measurement using the above columns\n",
    "    cpp_df['Sum_Coarse'] = cfa_data[col_list].sum(axis = 1)\n",
    "    # Check for negative counts\n",
    "    if min(cpp_df['Sum_Coarse']) < 0: \n",
    "        print('CPP function found negative sum of coarse particles.')\n",
    "    \n",
    "    # Remove rows with 0 sum_all counts. Don't divide by 0\n",
    "    cpp_df = cpp_df[cpp_df['Sum_All'] > 0]\n",
    "\n",
    "    # Return a series of the percent of particles that are coarse per row\n",
    "    return(cpp_df['Sum_Coarse'] / cpp_df['Sum_All'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a list of rows within dust events\n",
    "# Inputs: CFA data, dust event dataframe with depth intervals\n",
    "# Output: List of rows within dust events\n",
    "\n",
    "def label_dust_events(cfa_data, dust_depths):\n",
    "    \n",
    "    # Create an empty list to record the CFA measurements within depth range of dust events\n",
    "    dust_event_true = []\n",
    "    \n",
    "    # Subset the CFA data for depths within range of dust events\n",
    "    for index, row in dust_events.iterrows():\n",
    "        new_cfa = cfa_data[(cfa_data['Depth (m)'] >= row['Dust Event Start (m)']) &\n",
    "                           (cfa_data['Depth (m)'] <= row['Dust Event End (m)'])]\n",
    "        # Check that there are CFA measurements in the dust event depth intervals\n",
    "        if new_cfa.empty: continue\n",
    "        # Add all indices within dust events to the list\n",
    "        else:\n",
    "            dust_event_true.extend(new_cfa.index.values.tolist())\n",
    "            \n",
    "    # Return list of rows within dust events \n",
    "    return dust_event_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies CFA measurements with hump PSD anomalies\n",
    "# Inputs: CFA data and the min/max depths in which to find the humps\n",
    "# Output: CFA dataframe with only the hump measurements\n",
    "# Hump criteria: Measurements where all bins 3.5-10 um have higher counts than the\n",
    "#    average count for bins 1.5-2.9 um.\n",
    "\n",
    "def find_humps(cfa_data, min_depth, max_depth):\n",
    "    \n",
    "    # Subset the CFA data for the selected depth range\n",
    "    cfa_data = cfa_data[(cfa_data['Depth (m)'] >= min_depth) & \n",
    "                        (cfa_data['Depth (m)'] <= max_depth)]\n",
    "    \n",
    "    # Make a copy of CFA data for bins 3.2-10\n",
    "    humps_col_list = ['3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10']\n",
    "    \n",
    "    cfa_humps = cfa_data[humps_col_list]\n",
    "    \n",
    "    # Make copy of CFA data for bins 1.5-2.9\n",
    "    smalls_col_list = ['1.5', '1.6', '1.7', '1.8', '1.9', '2', '2.1', '2.2',\n",
    "                       '2.3', '2.4', '2.5', '2.7', '2.9']\n",
    "    \n",
    "    cfa_smalls = cfa_data[smalls_col_list]  \n",
    "    \n",
    "    # Get mean concentration for bins 1.5-2.9 per row\n",
    "    smalls_mean = cfa_smalls.mean(axis = 'columns')\n",
    "    \n",
    "    # Subtract the 1.5-2.9 bin mean from the 3.2-10 values\n",
    "    cfa_humps = cfa_humps.subtract(smalls_mean, axis = 'index')\n",
    "    \n",
    "    # If all subtracted values are positive, it is a hump\n",
    "    # Mark all positive differences as True\n",
    "    criteria = cfa_humps > 0\n",
    "    # Check for rows with only True values\n",
    "    criteria = criteria.all(axis = 'columns')\n",
    "\n",
    "    # Subset the CFA data for rows where all values are elevated above small particles\n",
    "    cfa_data = cfa_data[criteria]\n",
    "    \n",
    "    # Count the number of discrete humps\n",
    "    # Copy the first two columns of the CFA data into new dataframe\n",
    "    humps_diff = cfa_data.loc[:, 'Depth (m)':'ECM'].copy()\n",
    "    # Subtract each row from the one before (to get diff in depth)\n",
    "    humps_diff = humps_diff.diff()\n",
    "    # Subset the diff dataframe for rows where depth changes by 3+ cm\n",
    "    # ~3 cm melt resolution. >3 cm diff = new hump event\n",
    "    new_humps = humps_diff[(humps_diff['Depth (m)'] >= 0.03)]\n",
    "    \n",
    "    # Report the number of hump measurements and events\n",
    "    choice = input('Print detailed hump anomaly counts? Enter Y or N: ')\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        print('Number of measurements:    ', len(cfa_data))\n",
    "        print('Number of discrete events: ', len(new_humps))\n",
    "    \n",
    "    return cfa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Median Absolute Deviation (MAD)\n",
    "# Inputs: One-dimensional dataset (like CFA particle concentration or CPP)\n",
    "# Output: MAD\n",
    "\n",
    "def median_absolute_deviation(x, axis = None):\n",
    "    # Get the absolute value of every element minus the overall mean\n",
    "    deviation = abs(x - x.median())\n",
    "    \n",
    "    # Return the median of that deviation\n",
    "    return deviation.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers given different background & sensitivity conditions\n",
    "# Inputs: CFA data, list of dust event rows, list of volcanic event rows, background window size, MAD threshold\n",
    "# Outputs: Outlier counts, dataframe with overlapping outliers removed, number of outliers removed\n",
    "\n",
    "def remove_outliers_MAD(cfa_data, dust_indices, volc_indices, background_interval, threshold):\n",
    "    print('\\nRemoving MAD outliers')\n",
    "    \n",
    "    # Calculate rolling medians and overall median absolute deviation (MAD)\n",
    "    # Will calculate if 3 measurements in the window that aren't NaN\n",
    "    \n",
    "    cpp_background  = cfa_data['CPP'].rolling(background_interval, min_periods = 3).median()\n",
    "    conc_background = cfa_data['Sum 1.1-10'].rolling(background_interval, min_periods = 3).median()\n",
    "    \n",
    "    cpp_mad  = median_absolute_deviation(cfa_data['CPP'])\n",
    "    conc_mad = median_absolute_deviation(cfa_data['Sum 1.1-10'])\n",
    "\n",
    "    # Subsetting CFA data for the outliers\n",
    "    # Point is an outlier if it exceeds threshold * MAD from the background\n",
    "    cpp_peaks  = cfa_data[(cfa_data['CPP']        >= (cpp_background  + threshold * cpp_mad))]\n",
    "    conc_peaks = cfa_data[(cfa_data['Sum 1.1-10'] >= (conc_background + threshold * conc_mad))]\n",
    "\n",
    "    # Want to find when these outliers occur at the same time\n",
    "    overlap = conc_peaks.index.intersection(cpp_peaks.index)\n",
    "    # Prevent rows in real dust events from being removed\n",
    "    overlap = overlap.difference(dust_indices)\n",
    "    \n",
    "    # Ask the user whether or not to preserve outliers at volcanic events\n",
    "    choice1 = input('\\n-->Remove outliers at volcanic events? Enter Y or N: ')\n",
    "    # Ask the user whether or not to display # of outliers found & removed\n",
    "    choice2 = input('\\n-->Print results? Enter Y or N: ')\n",
    "    \n",
    "    if choice1 == 'y' or choice1 == 'Y':\n",
    "        # Remove variable has the indices at which to NaN values\n",
    "        remove = overlap\n",
    "        if choice2 == 'y' or choice2 == 'Y':\n",
    "            print('\\nSUMMARY OF OUTLIER REMOVAL')\n",
    "            print('1) Rows Removed: ', len(remove))\n",
    "    if choice1 == 'n' or choice1 == 'N':\n",
    "        # Subtract the volcanic event indices from the overlapping outlier indices\n",
    "        remove = overlap.difference(volc_indices)\n",
    "        if choice2 == 'y' or choice2 == 'Y':\n",
    "            print('\\nSUMMARY OF OUTLIER REMOVAL')\n",
    "            print('1) Rows Removed: ', len(remove))\n",
    "    \n",
    "    # Count number of discrete volcanic events in the rows we're about to remove\n",
    "    # Get a copy of the CFA data with only the 'New Volcanic Event?' column\n",
    "    temp_cfa = cfa_data.loc[remove, :].copy()\n",
    "    volc_event = temp_cfa[(temp_cfa['New Volcanic Event?'] == True)]\n",
    "    if choice2 == 'y' or choice2 == 'Y':\n",
    "        print('2) Number of discrete volcanic events in removed rows:', len(volc_event))\n",
    "        \n",
    "    # Count number of core breaks in the rows we're about to remove\n",
    "    temp_cfa = cfa_data.loc[remove, :].copy()\n",
    "    break_outliers = temp_cfa[(temp_cfa['New Break?'] == True)]\n",
    "    if choice2 == 'y' or choice2 == 'Y':\n",
    "        print('3) Number of discrete core breaks in removed rows:    ', len(break_outliers))\n",
    "        \n",
    "    # Change all rows with overlapping outliers to NaN\n",
    "    # Don't NaN the 'Break?', 'New Break?', 'Volcanic Event?', and 'New Volcanic Event?' columns\n",
    "    cfa_data.loc[remove, ['Depth (m)', 'Age b 1950', 'Flow Rate', 'ECM', '1', '1.1',\n",
    "                          '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9', '2', \n",
    "                          '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', '3.2', '3.6', \n",
    "                          '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', '9', '10', '12',\n",
    "                          'CPP', 'Sum 1.1-10']] = np.nan\n",
    "\n",
    "    return cfa_data, len(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create 1 bar plot of particle counts within x-centimenters of a point\n",
    "#     Inputs: CFA dataframe, depth of interest, depth range around that point\n",
    "#     Output: Bar plot of particle counts per bin, with option to save\n",
    "\n",
    "def plot_single_psd(cfa_data, point, depth_range):\n",
    "\n",
    "    # The range around the main point of interest\n",
    "    point_min = point - depth_range\n",
    "    point_max = point + depth_range\n",
    "\n",
    "    # Subset the CFA data to range around given point\n",
    "    point_cfa = cfa_data[(cfa_data['Depth (m)'] >= point_min) \n",
    "                        & (cfa_data['Depth (m)'] <= point_max)]\n",
    "    \n",
    "    # Check for and remove columns\n",
    "    col_list = ['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "                    '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9', \n",
    "                    '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "                    '9', '10']\n",
    "    point_cfa = point_cfa[col_list]\n",
    "    \n",
    "    # Sum particles by column around main & comp. points\n",
    "    point_count = point_cfa.sum(axis = 0)\n",
    "    \n",
    "    # Make figures\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "    \n",
    "    ax.bar(col_list, point_count, width = 1, color = 'black');\n",
    "    ax.set_xticks([0,10,20,29])\n",
    "    ax.tick_params(labelsize = 14)\n",
    "    ax.set_ylabel('Counts (#/uL)', fontsize = 16)\n",
    "    ax.set_title(str(point) + ' Meters +/- ' + str(depth_range) + ' Meters', fontsize = 18)    \n",
    "    \n",
    "    choice = input('Save Figure? Enter Y or N: ')\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        os.chdir('C:\\\\Users\\\\katie\\\\OneDrive\\\\Documents\\\\SPICE\\\\PSD Plots')\n",
    "        plt.savefig('PSD_1_' + str(point) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print summary statistics for dust concentration & CPP during data cleaning\n",
    "#     Inputs: CFA dataframe with particle sum and CPP columns\n",
    "#     Output: None. Prints summary statistics.\n",
    "\n",
    "def summary_statistics(cfa_data):\n",
    "    \n",
    "    if 'Sum 1.1-10' in cfa_data.columns and 'CPP' in cfa_data.columns:  \n",
    "        # Make local copies of particle concentration & CPP\n",
    "        row_sums = cfa_data.loc[:, 'Sum 1.1-10'].copy()\n",
    "        cpp      = cfa_data.loc[:, 'CPP'].copy()\n",
    "        \n",
    "        # Convert number concentration to # / mL\n",
    "        number_conc = row_sums.mul(1000).copy()\n",
    "        \n",
    "        # Call function to calculate MAD for the particle concentration & CPP\n",
    "        conc_mad = median_absolute_deviation(cfa_data['Sum 1.1-10'])\n",
    "        cpp_mad  = median_absolute_deviation(cfa_data['CPP'])\n",
    "    \n",
    "        print('SUMMARY STATISTICS')\n",
    "        # Skip NaNs when calculating these\n",
    "        print('Dust number concentration (/mL):')\n",
    "        print('    Mean:   %.2f' % (np.nanmean(number_conc)))\n",
    "        print('    Median: %.2f' % (np.nanmedian(number_conc)))\n",
    "        print('    Min:    %.2f' % (np.nanmin(number_conc)))\n",
    "        print('    Max:    %.2f' % (np.nanmax(number_conc)))\n",
    "        print('    StDev:  %.2f' % (np.nanstd(number_conc)))\n",
    "        print('    MAD:    %.2f' % conc_mad)\n",
    "    \n",
    "        print('\\nCoarse Particles (%):')\n",
    "        print('    Mean:   %.2f' % (np.nanmean(cpp)))\n",
    "        print('    Median: %.2f' % (np.nanmedian(cpp)))\n",
    "        print('    Min:    %.2f' % (np.nanmin(cpp)))\n",
    "        print('    Max:    %.2f' % (np.nanmax(cpp)))\n",
    "        print('    StDev:  %.2f' % (np.nanstd(cpp)))\n",
    "        print('    MAD:    %.2f' % cpp_mad)\n",
    "        \n",
    "    else: print('Input data with particle sum and CPP columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers using rolling 2-pt integrals (Aaron)\n",
    "# Inputs: CFA data, stdev threshold above median, list of dust event rows, list of volcanic rows\n",
    "# Output: List of rows to remove from the CFA data\n",
    "\n",
    "def remove_outliers_integrals(cfa_data, threshold, dust_indices, volc_indices):\n",
    "    print('\\nRemoving integral outliers')\n",
    "    # Calculate thresholds for concentration & CPP outliers\n",
    "    \n",
    "    # Lists for integral values\n",
    "    # y is the integral size\n",
    "    y = 2\n",
    "    conc_tpz = []\n",
    "    cpp_tpz  = []\n",
    "    \n",
    "    for x in range(0, len(cfa_data), 2):\n",
    "        # Concentration integrals\n",
    "        # Make sure neither value is NaN. All NaN values in concentration column should be NaN in CPP column\n",
    "        if cfa_data.loc[x, 'Sum 1.1-10'] == np.nan or cfa_data.loc[y, 'Sum 1.1-10'] == np.nan: \n",
    "            continue\n",
    "        else:\n",
    "            conc_tpz.append(trapz(cfa_data['Sum 1.1-10'][x: (x+y)]))\n",
    "            # CPP integrals\n",
    "            cpp_tpz.append(trapz(cfa_data['CPP'][x: (x+y)]))\n",
    "               \n",
    "    conc_stdev      = np.nanstd(conc_tpz)\n",
    "    conc_median     = np.nanmedian(conc_tpz)\n",
    "    # Threshold for error\n",
    "    conc_error = (threshold * conc_stdev) + conc_median\n",
    "\n",
    "    cpp_stdev      = np.nanstd(cpp_tpz)\n",
    "    cpp_median     = np.nanmedian(cpp_tpz)\n",
    "    # Threshold for error\n",
    "    cpp_error  = (threshold * cpp_stdev) + cpp_median\n",
    "                        \n",
    "    # Concentration outliers\n",
    "    y = 2\n",
    "    conc_outlier_indices = []\n",
    "    cpp_outlier_indices  = []\n",
    "    \n",
    "    for x in range(0,len(cfa_data), 2):\n",
    "    \n",
    "        # Concentration outliers\n",
    "        if trapz(cfa_data['Sum 1.1-10'].iloc[x:y]) >= conc_error:  # If area increases outside threshold\n",
    "            # Only append indices that aren't already in the list. Avoids duplicates.\n",
    "            if x not in conc_outlier_indices:\n",
    "                conc_outlier_indices.append(x)\n",
    "            if y not in conc_outlier_indices:\n",
    "                conc_outlier_indices.append(y)\n",
    "            \n",
    "        # CPP outliers        \n",
    "        if trapz(cfa_data['CPP'].iloc[x:y]) >= cpp_error:  # If area increases outside threshold\n",
    "            if x not in cpp_outlier_indices:\n",
    "                cpp_outlier_indices.append(x)\n",
    "            if y not in cpp_outlier_indices:\n",
    "                cpp_outlier_indices.append(y)\n",
    "        y = y + 2\n",
    "\n",
    "    # Convert index lists to sets to find the overlap between them\n",
    "    conc_outlier_set = set(conc_outlier_indices)\n",
    "    cpp_outlier_set  = set(cpp_outlier_indices)\n",
    "\n",
    "    # Find overlapping indices with both concentration & CPP outliers\n",
    "    overlap = conc_outlier_set.intersection(cpp_outlier_set)\n",
    "    # Convert to list again and sort\n",
    "    overlap = list(overlap)\n",
    "    overlap.sort()\n",
    "    \n",
    "    # Select those rows in the CFA data\n",
    "    cfa_data = cfa_data.loc[overlap, :]\n",
    "    # Prevent real dust & volc. events from being removed\n",
    "    remove = cfa_data.index.difference(dust_indices)\n",
    "    remove = cfa_data.index.difference(volc_indices)\n",
    "\n",
    "    # Return list of outlier rows and the rows to remove\n",
    "    return overlap, remove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
