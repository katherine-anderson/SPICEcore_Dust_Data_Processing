{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "#                        SPICE REMOVE ERRORS SCRIPT 3.0\n",
    "# Removes MEASUREMENT errors in the raw CFA data, labels CORE BREAKS & VOLCANIC EVENTS\n",
    "#\n",
    "#    - Loads raw, unfiltered CFA with depth corrections\n",
    "#    - NaNs bubbles and liquid conductivity values < 0.6\n",
    "#    - NaNs measurements without positive flow rates\n",
    "#    - NaNs negative Abakus values\n",
    "#    - NaNs measurements with depth duplicates or decreases\n",
    "#    - Tracks the number of measurements NaN'ed in each step\n",
    "#    - Labels all measurements within specified depth ranges of core breaks\n",
    "#    - Creates timescale for CFA data (annual in Holocene, tie points for glacial)\n",
    "#    - Labels all measurements within specific years of volcanic events\n",
    "#    - Labels the starting row for each volcanic event\n",
    "#    - Labels all measurements within known dust events\n",
    "#    - Calculates particle concentration and CPP\n",
    "#    - Exports cleaned dataset to CSV\n",
    "#\n",
    "# Katie Anderson, 8/4/19\n",
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   scipy.io import loadmat\n",
    "from   scipy    import interpolate\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from   datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "# Go into the correct directory- this is a personal desktop folder that backs up to OneDrive\n",
    "# Current setup: Master SPICE folder with scripts, data, and figure subfolders\n",
    "os.chdir('C:\\\\Users\\\\katie\\\\OneDrive\\\\Documents\\\\SPICE\\\\Scripts')\n",
    "\n",
    "# Run script to establish function definitions\n",
    "%run \"SPICE Data Processing Functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------CFA FILE PREP--------------------------#\n",
    "\n",
    "# Load Matlab raw, unfiltered, depth-corrected CFA file\n",
    "mat = loadmat('../Data/CFA_Unfiltered_Synchronized_7_24_19.mat')\n",
    "mdata = mat['FinalCFA']\n",
    "\n",
    "# Create dataframe and add 1st column\n",
    "cfa = pd.DataFrame({'Depth (m)':mdata[:,0]})\n",
    "# Add remaining columns and data to the dataframe\n",
    "cfa['Flow Rate'] = mdata[:,1]\n",
    "cfa['ECM'] = mdata[:,2]\n",
    "cfa['1']   = mdata[:,3]\n",
    "cfa['1.1'] = mdata[:,4]\n",
    "cfa['1.2'] = mdata[:,5]\n",
    "cfa['1.3'] = mdata[:,6]\n",
    "cfa['1.4'] = mdata[:,7]\n",
    "cfa['1.5'] = mdata[:,8]\n",
    "cfa['1.6'] = mdata[:,9]\n",
    "cfa['1.7'] = mdata[:,10]\n",
    "cfa['1.8'] = mdata[:,11]\n",
    "cfa['1.9'] = mdata[:,12]\n",
    "cfa['2']   = mdata[:,13]\n",
    "cfa['2.1'] = mdata[:,14]\n",
    "cfa['2.2'] = mdata[:,15]\n",
    "cfa['2.3'] = mdata[:,16]\n",
    "cfa['2.4'] = mdata[:,17]\n",
    "cfa['2.5'] = mdata[:,18]\n",
    "cfa['2.7'] = mdata[:,19]\n",
    "cfa['2.9'] = mdata[:,20]\n",
    "cfa['3.2'] = mdata[:,21]\n",
    "cfa['3.6'] = mdata[:,22]\n",
    "cfa['4']   = mdata[:,23]\n",
    "cfa['4.5'] = mdata[:,24]\n",
    "cfa['5.1'] = mdata[:,25]\n",
    "cfa['5.7'] = mdata[:,26]\n",
    "cfa['6.4'] = mdata[:,27]\n",
    "cfa['7.2'] = mdata[:,28]\n",
    "cfa['8.1'] = mdata[:,29]\n",
    "cfa['9']   = mdata[:,30]\n",
    "cfa['10']  = mdata[:,31]\n",
    "cfa['12']  = mdata[:,32]\n",
    "\n",
    "# Load corrected core breaks file\n",
    "breaks = pd.read_csv('../Data/Core Breaks Full Core.csv')\n",
    "\n",
    "# Load annual timescale for the Holocene\n",
    "holocene_timescale = pd.read_excel('../Data/Holocene Timescale.xlsx')\n",
    "\n",
    "# Load tie point timescale for the glacial\n",
    "glacial_timescale = pd.read_excel('../Data/Glacial Tie Point Timescale.xlsx')\n",
    "\n",
    "# Load combined Holocene and glacial volcanic records\n",
    "\n",
    "# These are the 3*MAD from Dave, 101-pt running median\n",
    "volcanic_record = pd.read_excel('../Data/Volcanic Record.xlsx')\n",
    "# Select the glacial rows, which don't yet have ages\n",
    "glacial_volc = volcanic_record[volcanic_record['Volcanic Depth (m)'].notnull()].copy()\n",
    "# Interpolate ages for the glacial volcanic events\n",
    "glacial_volc_age_interp = pd.Series(np.interp(glacial_volc['Volcanic Depth (m)'], \n",
    "                                              glacial_timescale['Bot D (m)'], glacial_timescale['Bot Year (b1950)']))\n",
    "glacial_volc['Start Year (b1950)'] = glacial_volc_age_interp.values\n",
    "# Add glacial volcanic ages to the complete volcanic record list\n",
    "volcanic_record.loc[1209:5400, 'Start Year (b1950)'] = glacial_volc['Start Year (b1950)']\n",
    "\n",
    "# Load depths of real dust events\n",
    "dust_events = pd.read_excel('../Data/Dust Events.xlsx')\n",
    "# Only keep the columns with the depth ranges\n",
    "dust_events = dust_events.loc[:, 'Dust Event Start (m)':'Dust Event End (m)'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CFA dataset length: 448668\n",
      "\n",
      "Filtering liquid conductivity, flow rate, Abakus, and depth data.\n",
      "\n",
      "Bubble errors:                3804\n",
      "Liquid conductivity < 0.6:    2664\n",
      "No/negative flow rate errors: 915\n",
      "Depth not increasing errors:  3074\n",
      "\n",
      "Removing negative Abakus values.\n",
      "\n",
      "Labelling core breaks.\n",
      "\n",
      "Interpolating timescale.\n",
      "\n",
      "Labelling volcanic events.\n",
      "\n",
      "Labelling dust events.\n",
      "\n",
      "Calculating particle concentration and CPP.\n",
      "-->Use smallest and largest bins for CPP? Enter Y or N: n\n",
      "\n",
      "Final CFA dataset length: 438211\n",
      "\n",
      "Data exported to CSV.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                                        PART 1:\n",
    "#                   CFA DATA FILTERING AND MECHANICAL ERROR REMOVAL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "original_length = len(cfa)\n",
    "print('Original CFA dataset length:', original_length)\n",
    "print('\\nFiltering liquid conductivity, flow rate, Abakus, and depth data.\\n')\n",
    "\n",
    "# 1) Remove data reflecting bubbles with ECM \n",
    "\n",
    "#    Do this before NaN'ing a bunch of rows\n",
    "#    DOM AND AARON DEFINE BUBBLES DIFFERENTLY. DOM: < 90% OF LAST/NEXT. AARON: STEEP +/- SLOPES (25)\n",
    "\n",
    "#    Loop through the data and NaN all rows where slopes indicate bubbles. Remove these rows.\n",
    "threshold_bubbles = 25\n",
    "bubbles = 0\n",
    "for i in range(1, len(cfa['Depth (m)']) - 1):                      \n",
    "    # Calculate the slope between the ECM at index i and the points before and after it\n",
    "    if (cfa['Depth (m)'][i] - cfa['Depth (m)'][i - 1]) == 0 or (cfa['Depth (m)'][i + 1] - cfa['Depth (m)'][i]) == 0: \n",
    "        continue # Don't divide by 0\n",
    "    else:\n",
    "        slope1 = (cfa['ECM'][i] - cfa['ECM'][i - 1]) / (cfa['Depth (m)'][i] - cfa['Depth (m)'][i - 1])\n",
    "        # No need to do the other calculations if the slope with the point before is above threshold\n",
    "        if slope1 <= -threshold_bubbles:  \n",
    "            slope2 = (cfa['ECM'][i + 1] - cfa['ECM'][i]) / (cfa['Depth (m)'][i + 1] - cfa['Depth (m)'][i]) \n",
    "            if slope2 >= threshold_bubbles: # If slope1 <= 0 and slope2 >= 0, it's a bubble                       \n",
    "                bubbles = bubbles + 1\n",
    "                cfa.loc[i] = np.nan # Change all values in row to NaN\n",
    "print('Bubble errors:               ', bubbles)\n",
    "\n",
    "# 2) Filter out data with ECM values < 0.6\n",
    "\n",
    "# Drop all good rows, where ECM is >= 0.6, and count remaining bad rows\n",
    "bad_ecm = cfa.drop(cfa[cfa['ECM'] >= 0.6].index)\n",
    "ecm = len(bad_ecm)\n",
    "\n",
    "# Change all values in rows with ECM < 0.6 to NaN\n",
    "bad_rows = list(bad_ecm.index.values)\n",
    "# Change values in the bad depth rows to NaN\n",
    "cfa.loc[bad_rows, :] = np.nan\n",
    "\n",
    "print('Liquid conductivity < 0.6:   ', ecm - bubbles)\n",
    "\n",
    "# 3) Filter out data without positive flow rate values\n",
    "\n",
    "# Drop all good rows, where flow rate isn't > 0, and count remaining bad rows\n",
    "bad_flow = cfa.drop(cfa[cfa['Flow Rate'] > 0].index)\n",
    "flow = len(bad_flow)\n",
    "# Change all values in rows without positive flow rates to NaN\n",
    "bad_rows = list(bad_flow.index.values)\n",
    "# Change values in the bad depth rows to NaN\n",
    "cfa.loc[bad_rows, :] = np.nan\n",
    "\n",
    "print('No/negative flow rate errors:', flow - ecm)\n",
    "\n",
    "# 4) Filter out measurements where depth does not increase\n",
    "\n",
    "# Subtract each row from the one before\n",
    "# All NaNs stay NaN\n",
    "diff = cfa.diff(periods = 1, axis = 0)\n",
    "# Drop all good rows, where the diff is > 0\n",
    "bad_depth = diff.drop(diff[diff['Depth (m)'] > 0].index)\n",
    "# Delete all rows with NaN and count remaining bad rows\n",
    "bad_depth = bad_depth.dropna()\n",
    "depth = len(bad_depth)\n",
    "\n",
    "print('Depth not increasing errors: ', depth)\n",
    "\n",
    "# Get a list of all of the indices with bad depth measurements\n",
    "bad_rows = list(bad_depth.index.values)\n",
    "# Change values in the bad depth rows to NaN\n",
    "cfa.loc[bad_rows, :] = np.nan\n",
    "\n",
    "# 5) Filter out any inf. or negative Abakus values. Check that everything is NaN'd.\n",
    "\n",
    "print('\\nRemoving negative Abakus values.')\n",
    "# Get rid of any individual inf. values (just in case)\n",
    "cfa = cfa.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Get a copy of just the Abakus columns\n",
    "abakus = cfa.loc[:,'1':'12'].copy()\n",
    "# NaN all negative values (just the values, not the entire row)\n",
    "abakus[abakus < 0] = np.nan\n",
    "# Replace CFA abakus columns with corrected abakus columns\n",
    "cfa.loc[:, '1':'12'] = abakus\n",
    "\n",
    "# Make sure all NaN'd depths have NaN'd CFA data\n",
    "depth_isnull = cfa['Depth (m)'].isnull()\n",
    "bad_rows = depth_isnull[depth_isnull == True].index.values\n",
    "cfa.loc[bad_rows, :] = np.nan\n",
    "\n",
    "# 6) Label each CFA row near core breaks\n",
    "\n",
    "print('\\nLabelling core breaks.')\n",
    "\n",
    "# Add Y/N 'Break?' column. Default to False.\n",
    "cfa['Break?'] = False\n",
    "# Add Y'N 'New Break?' column to record first row in each discrete core break range. Default False.\n",
    "cfa['New Break?'] = False\n",
    "\n",
    "# Get the row indices of all measurements near core breaks\n",
    "# Inputs: CFA data, break list, depth range\n",
    "break_rows, new_break_rows = label_core_breaks(cfa, breaks, 0.03)\n",
    "# Change all 'Break?' values in those rows to True\n",
    "cfa.loc[break_rows, 'Break?'] = True\n",
    "cfa.loc[new_break_rows, 'New Break?'] = True\n",
    "\n",
    "#7) Interpolate ages for the CFA rows\n",
    "\n",
    "# Need to do this before adding in the volcanic events\n",
    "print('\\nInterpolating timescale.')\n",
    "\n",
    "# Subset the Holocene CFA data\n",
    "# This is the row where depths < 798 m (closest to end of annual timescale)\n",
    "# Make a copy of this data to avoid chained assignment later\n",
    "cfa_holocene = cfa.loc[0:214959, :].copy()\n",
    "\n",
    "# Interpolate ages for Holocene annual timescale\n",
    "age_interp = pd.Series(np.interp(cfa_holocene['Depth (m)'], \n",
    "                                 holocene_timescale['Depth (m)'], holocene_timescale['Age (yr b 1950)']))\n",
    "# Save ages to copy of Holocene CFA. Keep until step 8) is complete for the whole core\n",
    "cfa_holocene['Age b 1950'] = age_interp.values\n",
    "\n",
    "# Subset the deep CFA data\n",
    "# Start with the next row after the Holocene rows\n",
    "cfa_deep = cfa.loc[214960:448667, :].copy()\n",
    "\n",
    "# Interpolate ages for the deep core tie points\n",
    "glacial_age_interp = pd.Series(np.interp(cfa_deep['Depth (m)'],\n",
    "                                         glacial_timescale['Bot D (m)'], glacial_timescale['Bot Year (b1950)']))\n",
    "\n",
    "# Create one series with the interpolated age for each row\n",
    "ages = age_interp.append(glacial_age_interp)\n",
    "# Add 'Age' column with age values\n",
    "cfa['Age b 1950'] = ages.values\n",
    "\n",
    "#8) Label all measurements near volcanic events and dust events\n",
    "\n",
    "print('\\nLabelling volcanic events.')\n",
    "\n",
    "# Create Y/N 'Volcanic Event?' column. Default to False\n",
    "cfa['Volcanic Event?'] = False\n",
    "cfa['New Volcanic Event?'] = False\n",
    "\n",
    "# Get list of all indices occurring near volcanic events (by year, not depth)\n",
    "# Minus 2- and plus 2-year buffers\n",
    "volc_rows, new_event_rows = label_volc_events(cfa, volcanic_record, 2, 6)\n",
    "# Change all 'Volcanic Event?' values in those rows to True\n",
    "cfa.loc[volc_rows, 'Volcanic Event?'] = True\n",
    "cfa.loc[new_event_rows, 'New Volcanic Event?'] = True\n",
    "\n",
    "print('\\nLabelling dust events.')\n",
    "\n",
    "# Add Y/N 'Dust Event?' column. Default to false.\n",
    "cfa['Dust Event?'] = False\n",
    "# Get the row indices of all measurements within dust events\n",
    "dust_rows = label_dust_events(cfa, dust_events)\n",
    "# Change all 'Dust Event?' values in those rows to True\n",
    "cfa.loc[dust_rows, 'Dust Event?'] = True\n",
    "\n",
    "# 9) Calculate particle concentration and CPP, add to dataframe\n",
    "\n",
    "print('\\nCalculating particle concentration and CPP.')\n",
    "# Change this list if we decide to include smallest & largest bins\n",
    "sum_columns = ['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', \n",
    "               '1.9', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.7', '2.9',\n",
    "               '3.2', '3.6', '4', '4.5', '5.1', '5.7', '6.4', '7.2', '8.1', \n",
    "               '9', '10']\n",
    "# Set skipna to False, otherwise, rows with all NaNs will sum to 0\n",
    "cfa['Sum 1.1-10'] = cfa[sum_columns].sum(axis = 1, skipna = False)\n",
    "\n",
    "# Add CPP column to CFA dataframe. Function will ask to include/exclude bins 1 and 12\n",
    "cfa['CPP'] = find_cpp(cfa)\n",
    "\n",
    "# 10) Export CFA file to CSV. Report final length.\n",
    "print('\\nFinal CFA dataset length:', (len(cfa) - flow - depth))\n",
    "cfa.to_csv('../Data/Cleaned_CFA_Phase1_' + str(date.today()) + '.csv')\n",
    "      \n",
    "print('\\nData exported to CSV.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
